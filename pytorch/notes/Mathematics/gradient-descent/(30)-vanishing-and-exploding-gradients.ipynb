{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing and Exploding Gradients\n",
    "This document will cover:\n",
    "- About the vanishing and exploding gradient problems in Deep Learning (and backprop-optimization more generally)\n",
    "- A few strategies for avoiding these problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define (Vanishing and Exploding Gradient)\n",
    "- Vanishing Gradient:\n",
    "    - Weights don't change - No learning. Problematic for deep networks\n",
    "- Exploding Gradient:\n",
    "    - Weights chagnge wildly - Bad solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to minimize gradient problems\n",
    "- Use models with few hidden layers\n",
    "- Use activation functions that do not saturate (e.g., ReLU(Rectified Linear Unit))\n",
    "- Apply weight normilization\n",
    "- Pre-train networks using qutoencoders\n",
    "- Use regularization techniques like batch normilization, dropout, and weight decay\n",
    "- Use architectures like residual networks ('resnet')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
