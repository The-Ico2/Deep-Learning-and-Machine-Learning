{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Cross Entropy\n",
    "This document will cover:\n",
    "- How to interpret entropy\n",
    "- The formula of entropy\n",
    "- The main application of entropy in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Theoretic Entropy\n",
    "Entropy in physics: Matter in the universe naturally goes from a state of order to a state of disorder\n",
    "\n",
    "Shannon Entropy is a measure, its a quantity that describes the amount of uncertainty that we have about a specific variable.\n",
    "\n",
    "Shannon Entropy is also known as Information Theoretic Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Entropy and Cross-Entropy\n",
    "__Entropy Formula:__ H = -sum p(x) * log(p(x))\n",
    "- Describes one probability distribution\n",
    "- p is the probability\n",
    "- log is base-2 logarithm\n",
    "- Sum is there to flip negative-probability back to positive-probability\n",
    "    - Using log will return negative probability\n",
    "\n",
    "High entropy means that the dataset has a lot of variability\n",
    "\n",
    "Low entropy means tha most of the values in the dataset repeat (and therefore are redundant).\n",
    "\n",
    "How does entropy differ from variance?\n",
    "- Entropy is nonlinear and makes no assumptions about the distribution\n",
    "- Variance depends on the validity of the mean and therefore is appropriate for roughly normal data \n",
    "\n",
    "When using log base-2 the units are called \"bits\"\n",
    "When using natural log the units are called \"nats\"\n",
    "\n",
    "__Cross-Entropy Formula:__ H(p, q) = -sum p * log(q)\n",
    "- Describe relation between two probability distributions\n",
    "- p is the probability\n",
    "- log is base-2 logarithm\n",
    "- Sum is there to flip negative-probability back to positive-probability\n",
    "    - Using log will return negative probability so we use -sum to flip it back to positive\n",
    "\n",
    "|Entropy|Cross-Entropy|\n",
    "|:--:|:--:|\n",
    "|H(p) = -sum p * log(p)|H(p, q) = -sum p * log(q)|\n",
    "|describes one probability distribution|describes the relationship between two probability distribtuions|\n",
    "|High entropy means that the dataset has a lot of variability|High entropy means that the dataset has a lot of variability|\n",
    "|Low entropy means tha most of the values in the dataset repeat (and therefore are redundant).|Low entropy means tha most of the values in the dataset repeat (and therefore are redundant).|\n",
    "\n",
    "    (Note: In Deep Learning, events happen or don't happen --> p=0 or p=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5623351446188083\n",
      "0.43152310867767135\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# probability of event happening\n",
    "p = .25\n",
    "q = 1-p\n",
    "x = [.25, .75]\n",
    "\n",
    "H = 0\n",
    "for p in x:\n",
    "    H -= p*np.log(p)\n",
    "print(str(H))\n",
    "\n",
    "# written out for N = 2 events\n",
    "H = -(p*np.log(p)) + -(q*np.log(q))\n",
    "print(str(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "p1: 0.25 , q1: 0.75\n",
      "Entropy:  0.34657359027997264\n",
      "Binary Entropy:  0.5623351446188083\n",
      "\n",
      "p1: 0.24 , q1: 0.76\n",
      "Entropy:  0.342507925353635\n",
      "Binary Entropy:  0.5510799280869728\n",
      "\n",
      "p1: 0.28 , q1: 0.72\n",
      "Entropy:  0.3564303892276085\n",
      "Binary Entropy:  0.5929533174474745\n",
      "\n",
      "p1: 0.92 , q1: 0.07999999999999996\n",
      "Entropy:  0.07671108022392693\n",
      "Binary Entropy:  0.2787693717685873\n",
      "\n",
      "p2: 1 , q2: 0.25\n",
      "Cross-Entropy:  1.3862943611198906\n",
      "\n",
      "p2: 0 , q2: 0.75\n",
      "Cross-Entropy:  0.0\n",
      "Binary Cross-Entropy:  1.3862943611198906 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Entropy\n",
    "i = 0\n",
    "H = 0\n",
    "p1 = [.25, .24, .28, .92]\n",
    "while i < len(p1):\n",
    "    q1 = 1- p[i]\n",
    "\n",
    "    # Binary Entropy\n",
    "    H1a = -(p1[i]*np.log(p1[i])) + -(q1*np.log(q1))\n",
    "\n",
    "    # Normal Entropy\n",
    "    H1b = -(p1[i]*np.log(p1[i]))\n",
    "\n",
    "    # Printing Data\n",
    "    print('\\np1:', p1[i], ', q1:', q1)\n",
    "    print('Entropy: ', str(H1b))\n",
    "    print('Binary Entropy: ', str(H1a))\n",
    "    # incrementing\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Cross-Entropy\n",
    "i=0\n",
    "p2 = [1, 0]\n",
    "q2 = [.25, .75]\n",
    "while i < len(p2):\n",
    "\n",
    "    # Binary Cross-Entropy\n",
    "    H2a = -(p2[i]*np.log(q2[i]))\n",
    "    \n",
    "\n",
    "    # Printing Data\n",
    "    print('\\np2:', p2[i], ', q2:', q2[i])\n",
    "    print('Cross-Entropy: ', str(H2a))\n",
    "    # incrementing\n",
    "    i += 1\n",
    "H2b = -(p2[0]*np.log(q2[0])) + -(p2[1]*np.log(q2[1]))\n",
    "print('Binary Cross-Entropy: ', str(H2b), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3863)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# note: inputs MUST be a tensor\n",
    "q_tensor = torch.Tensor(q2)\n",
    "p_tensor = torch.Tensor(p2)\n",
    "\n",
    "# F.binary_cross_entropy(Predicted Prob(q), Prob(p))\n",
    "F.binary_cross_entropy(q_tensor, p_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
