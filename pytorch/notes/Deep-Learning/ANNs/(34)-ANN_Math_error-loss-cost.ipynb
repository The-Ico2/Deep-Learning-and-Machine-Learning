{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Math Part 2 (Errors, Loss, Cost)\n",
    "This document will cover:\n",
    "- More of the math underlying ANNs\n",
    "- Different categories of errors and their corresponding loss functions\n",
    "- The difference between loss and cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation vs Reality\n",
    "- __Binarized Error__ is easier to interpret, but less sensitive\n",
    "- __Continuous Error__ is more sensitive, but is signed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "|Mean-Squared Error (MSE)|Cross Entropy|\n",
    "|--|--|\n",
    "|Used for continuous data when the output is a numerical prediction|Use for categorical data when the model outputs a probability|\n",
    "|e.g., height, house price, temperature|e.g., presence of disease, animal in picture, text sentiment|\n",
    "|$L=\\frac{1}{2}(\\hat{y}-y)^{2}$|$L=-(y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y}))$|\n",
    "\n",
    "$W = \\frac{\\arg\\min J}{W}$\n",
    "\n",
    "$W = \\frac{1}{n}\\sum\\limits_{i=1}^{n}L(\\hat{y}_i,y_i)$\n",
    "\n",
    "$W = \\frac{1}{n}\\sum\\limits_{i=1}^{n}L(f(x, W)_i, y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is anything lost in the cost?\n",
    "$J=\\frac{1}{n}\\sum\\limits_{i=1}^{n}L(\\hat{y}_i,y_i)$\n",
    "\n",
    "__Why train on cost and not loss?__\n",
    "- Training on each sample is time-consuming and may lead to overfitting\n",
    "- But averaging over too many samples may decrease sensitivity\n",
    "- A good solution is to train the model in \"batches\" of samples"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
