{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs a similar experiment to the previous one, studying the interaction between learning rates and training epochs on finding the minimum of a function using gradient descent. However, this code is implemented using NumPy instead of PyTorch. Let's break down how it works:\n",
    "\n",
    "1. **Importing Libraries**: The script imports necessary libraries including NumPy (`np`) and Matplotlib (`plt`).\n",
    "\n",
    "2. **Setting Matplotlib Format**: Matplotlib's backend format is set to 'svg' to ensure that Matplotlib plots will be displayed in the Jupyter Notebook using the Scalable Vector Graphics (SVG) format.\n",
    "\n",
    "3. **Defining Functions**: \n",
    "   - A function `fx(x)` is defined, representing a mathematical function.\n",
    "   - A derivative function `deriv(x)` is defined, representing the derivative of the mathematical function.\n",
    "\n",
    "4. **Main Functionality**:\n",
    "   - The experiment involves varying learning rates and training epochs to find the minimum of the function using gradient descent.\n",
    "   - Parameters for learning rates and training epochs are set up using NumPy's `linspace` and `round` functions.\n",
    "   - A matrix `finalres` is initialized to store the results.\n",
    "   - It loops over learning rates and training epochs, performing gradient descent to find the minimum of the function.\n",
    "   - The final guess of the minimum for each combination of learning rate and training epochs is stored in the `finalres` matrix.\n",
    "   - The results are plotted using Matplotlib, showing the final guess of the minimum with respect to learning rates and training epochs. Two plots are created: one displaying the heatmap of the final guesses and another showing the final function estimate for each learning rate.\n",
    "\n",
    "Overall, this code performs a similar experiment to the previous one but using NumPy instead of PyTorch for numerical computation. Both codes aim to study the behavior of gradient descent under different learning rates and training epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
